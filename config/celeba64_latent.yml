train_dataset_config:
  name: "CELEBA64"
  data_path: "./data/celeba64"
  image_size: 64
  image_channel: 3
  latent_dim: &latent_dim 512
  split: "train"
  augmentation: False
eval_dataset_config:
  split: "valid"
  augmentation: False

trained_ddpm_config: "./pre-trained-dpms/celeba64/config.yml"
trained_representation_learning_config: "./trained-models/autoencoder/celeba64/config.yml"
trained_representation_learning_checkpoint: "./trained-models/autoencoder/celeba64/checkpoint.pt"
inferred_latents: "./trained-models/latents/celeba64.pt"

latent_denoise_fn_config:
  model: "CELEBA64LatentDenoiseFn"
  input_channel: *latent_dim
  model_channel: 2048
  num_layers: 10
  time_emb_channel: 64
  use_norm: True
  dropout: 0.0

dataloader_config:
  train:
    num_workers: 4
    batch_size: 128 # batch_size for each process
  eval:
    num_generations: 36 # showing a 6x6 image grid in tensorboard

optimizer_config:
  name: "AdamW"
  lr: 1e-3
  adam_betas: (0.9, 0.999)
  adam_eps: 1e-8
  weight_decay: 0.01
  enable_amp: False

runner_config:
  display_steps: 100
  evaluate_every_steps: 10000
  save_latest_every_steps: 1000
  save_checkpoint_every_steps: 10000
  ema_every: 1
  ema_decay: 0.9999
  num_iterations: 1
  compile: False